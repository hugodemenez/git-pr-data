diff --git a/.gitignore b/.gitignore
index 2fd10d1..e26fa9d 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,5 +1,6 @@
 .venv
-cloned_repo
+cloned_repo/
 __pycache__
 test.ipynb
-.ipynb_checkpoints
\ No newline at end of file
+.ipynb_checkpoints
+*.env
\ No newline at end of file
diff --git a/analysis/ML_analyser.py b/analysis/ML_analyser.py
deleted file mode 100644
index 7756924..0000000
--- a/analysis/ML_analyser.py
+++ /dev/null
@@ -1,35 +0,0 @@
-import os
-import logging
-from openai import OpenAI
-from typing import List
-
-logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
-logger = logging.getLogger(__name__)
-
-class ChatGPTApi:
-    """Class that is used to call chatgpt, you need to have your openai API key as an environemnt variable named OPENAI_API_KEY"""
-    def __init__(self) -> None:
-        assert os.getenv("OPENAI_API_KEY") is not None ,"No API key detected, please setup your API key as an environement variable under the name OPENAI_API_KEY"
-        self.client = OpenAI()
-        self.message = [
-            {"role": "system", "content": "You are a helpful assistant that is an expert at detecting mistakes in code."}
-            ]
-
-
-    def call(self, code=None):
-        if code is None or code == "":
-            return None
-        message = self.message
-        message.append({"role": "user", "content": f"In this code, tell me which lines are susceptible to throw an error.\n {code} Give only :  the line number, a short comment about why it is not good"})
-
-        response = self.client.chat.completions.create(
-        model="gpt-3.5-turbo-0125",
-        response_format={ "type": "json_object" },
-        messages=message
-        )
-        return response
-        # return response.choices[0].message.content
-
-    def call_on_list(self,code_list : List[str]):
-        for code in code_list:
-            yield self.call(code)
diff --git a/analysis/files_analyser.py b/analysis/files_analyser.py
deleted file mode 100644
index 3d00503..0000000
--- a/analysis/files_analyser.py
+++ /dev/null
@@ -1,45 +0,0 @@
-from pathlib import Path
-from typing import List
-from collections import Counter
-import pandas as pd
-from analysis.ML_analyser import ChatGPTApi
-
-import logging
-logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
-logger = logging.getLogger(__name__)
-
-
-config_path = "analysis/config/supported_extensions.csv"
-
-def get_important_extensions(list_files : List[Path] , extensions = []) -> List[str]:
-
-    if not extensions:
-        suffix_counts = Counter(map(lambda x : x.suffix ,list_files))
-        most_common_suffix_list = suffix_counts.most_common()
-        idx = 0
-        df = pd.read_csv(config_path)
-        while df[df['extension'] == most_common_suffix_list[idx][0]].shape[0] <= 0 and idx < len(most_common_suffix_list):
-            idx +=1
-        if idx >= len(most_common_suffix_list):
-            return []
-        most_common_suffix, _ = most_common_suffix_list[idx]
-        language = df[df['extension'] == most_common_suffix].iloc[0]['name']
-        return df[df['name'] == language]['extension'].to_list(),language
-    else:
-        liste = []
-        for suffix in extensions:
-            language = df[df['extension'] == suffix].iloc[0]['name']
-            liste += df[df['name'] == language]['extension'].to_list()
-        return list(set(map(lambda x : Path(x),liste))),language
-
-def get_file_analysis(list_files : List[Path]):
-    model = ChatGPTApi()
-    for file in list_files:
-        try:
-            with open(file,'r') as f:
-                content = map(lambda x : f"{x[0]} " + x[1],enumerate(f.readlines()))
-                code = "".join(content)
-                yield model.call(code) , len(content)
-        except:
-            logger.error(f"An error has occured for file : {file}")
-            yield None,None
\ No newline at end of file
diff --git a/app/__init__.py b/app/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/app/dependencies.py b/app/dependencies.py
new file mode 100644
index 0000000..e2a535c
--- /dev/null
+++ b/app/dependencies.py
@@ -0,0 +1,11 @@
+from typing import Annotated
+from fastapi import Header, HTTPException
+
+async def get_token_header(x_token: Annotated[str, Header()]):
+    if x_token != "fake-super-secret-token":
+        raise HTTPException(status_code=400, detail="X-Token header invalid")
+
+
+async def get_query_token(token: str):
+    if token != "App-Prove":
+        raise HTTPException(status_code=400, detail="No App-Prove token provided")
\ No newline at end of file
diff --git a/app/internal/__init__.py b/app/internal/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/app/main.py b/app/main.py
new file mode 100644
index 0000000..5f37ac1
--- /dev/null
+++ b/app/main.py
@@ -0,0 +1,53 @@
+import chardet
+import logging
+from fastapi import FastAPI, BackgroundTasks
+import uvicorn
+from utils import analysis
+from routers import ws
+from fastapi.middleware.cors import CORSMiddleware
+
+logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s', filename='app.log', filemode='w')
+logger = logging.getLogger(__name__)
+
+app = FastAPI()
+
+origins = [
+    "*",
+]
+
+app.add_middleware(
+    CORSMiddleware,
+    allow_origins=origins,
+    allow_credentials=True,
+    allow_methods=["*"],
+    allow_headers=["*"],
+)
+app.include_router(ws.repositories.router)
+
+
+def main(git_url: str):
+    clone_dir = "cloned_repo"
+    db_name = "file_data.db"
+    # Step 1: Clone the repository
+    analysis.clone_repo(git_url, clone_dir)
+    # Step 2: Process the repository to count files and lines
+    number_of_files,total_line_count,list_of_programming_languages,list_response = analysis.process_repo(clone_dir)
+    logger.debug(f"Number of files : {number_of_files}")
+    logger.debug(f"Total line count : {total_line_count}")
+    logger.debug(f"Most common programming languages : {list_of_programming_languages}")
+    logger.debug(f"Code which may throw error : {list_response}")
+
+    # Step 3: Store the data in a SQLite database
+    # store_data_in_db(db_name, data)
+
+    # logger.info(f"Processed {len(data)} files, representing {total_line_count} lines. Data stored in {db_name}.")
+    return
+
+@app.get("/")
+def read_root(git_url: str, background_tasks: BackgroundTasks):
+    logger.info("Processing repository...")
+    background_tasks.add_task(main, git_url)
+    return {"message": "Processing repository in the background"}
+
+if __name__ == "__main__":
+    uvicorn.run("main:app", host="0.0.0.0", port=8000, log_level="info", reload=True, reload_excludes=['cloned_repo/*'])
diff --git a/app/routers/__init__.py b/app/routers/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/app/routers/stream/__init__.py b/app/routers/stream/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/app/routers/stream/repositories.py b/app/routers/stream/repositories.py
new file mode 100644
index 0000000..67dd3e8
--- /dev/null
+++ b/app/routers/stream/repositories.py
@@ -0,0 +1,164 @@
+"""
+This file contains the code for the websocket endpoint.
+The websocket endpoint is used to create a websocket connection between the client and the server.
+The goal is to update the client frontend based on websocket messages created by events on the server.
+"""
+
+import json
+from pathlib import Path
+from fastapi import APIRouter, WebSocket, Depends
+from fastapi.responses import HTMLResponse, StreamingResponse
+
+from utils import analysis
+from utils.databases import store_data_in_db
+
+from dependencies import get_token_header
+import logging
+
+logger = logging.getLogger(__name__)
+
+CLONE_DIR: Path = "cloned_repo"
+DB_NAME: str = "file_data.db"
+
+router = APIRouter(
+    prefix="/stream/repositories",
+    tags=["repositories"],
+    # dependencies=[Depends(get_token_header)],
+    # responses={404: {"description": "Not found"}},
+)
+
+
+html = """
+<!DOCTYPE html>
+<html>
+    <head>
+        <title>Chat</title>
+    </head>
+    <body>
+        <h1>WebSocket Chat</h1>
+        <form action="" onsubmit="sendMessage(event)">
+            <input type="text" id="messageText" autocomplete="off"/>
+            <button>Send me</button>
+        </form>
+        <ul id='messages'>
+        </ul>
+        <script>
+            var ws = new WebSocket("ws://localhost:8000/repositories/ws/repository_url");
+            ws.onmessage = function(event) {
+                var messages = document.getElementById('messages')
+                var message = document.createElement('li')
+                var content = document.createTextNode(event.data)
+                message.appendChild(content)
+                messages.appendChild(message)
+            };
+            function sendMessage(event) {
+                var input = document.getElementById("messageText")
+                ws.send(input.value)
+                input.value = ''
+                event.preventDefault()
+            }
+
+
+// Create EventSource for SSE endpoint
+const eventSource = new EventSource('http://127.0.0.1:8000/stream/repositories/analysis/');
+
+eventSource.onopen = () => {
+    console.log('EventSource connected')
+    //Everytime the connection gets extablished clearing the previous data from UI
+    coordinatesElement.innerText = ''
+}
+
+//eventSource can have event listeners based on the type of event.
+//Bydefault for message type of event it have the onmessage method which can be used directly or this same can be achieved through explicit eventlisteners
+eventSource.addEventListener('locationUpdate', function (event) {
+    coords = JSON.parse(event.data);
+    console.log('LocationUpdate', coords);
+    updateCoordinates(coords)
+});
+
+//In case of any error, if eventSource is not closed explicitely then client will retry the connection a new call to backend will happen and the cycle will go on.
+eventSource.onerror = (error) => {
+    console.error('EventSource failed', error)
+    eventSource.close()
+}
+
+// Function to update and display coordinates
+function updateCoordinates(coordinates) {
+    // Create a new paragraph element for each coordinate and append it
+    const paragraph = document.createElement('p');
+    paragraph.textContent = `Latitude: ${coordinates.lat}, Longitude: ${coordinates.lng}`;
+    coordinatesElement.appendChild(paragraph);
+}
+
+        </script>
+    </body>
+</html>
+"""
+
+
+@router.get("/")
+async def get():
+    return HTMLResponse(html)
+
+
+@router.websocket("/ws")
+async def websocket_endpoint(websocket: WebSocket):
+    await websocket.accept()
+    while True:
+        data = await websocket.receive_text()
+        await websocket.send_text(f"Message text was: {data}")
+
+
+@router.get("/analysis/{repository_url}")
+async def ws_repository_analysis(repository_url: str):
+    """
+    Websocket endpoint for analysing a repository.
+    Submit data in realtime to the client, in order to update frontend.
+    """
+    def repository_analysis():
+        yield f"Connected to repository: {repository_url}"
+
+        # Step 1: Clone the repository
+        yield f"Cloning repository: {repository_url}"
+        analysis.clone_repo(repository_url, CLONE_DIR)
+        # Step 2: Process the repository to count files, lines, identify main languages
+        simple_repo_analysis = analysis.get_simple_repository_analysis(CLONE_DIR)
+        (
+            number_of_files,
+            total_line_count,
+            list_of_programming_languages,
+            ready_for_analysis,
+        ) = simple_repo_analysis
+
+        logger.debug(f"Number of files : {number_of_files}")
+        logger.debug(f"Total line count : {total_line_count}")
+        logger.debug(f"Most common programming languages : {list_of_programming_languages}")
+        # > Send data to the client
+        yield json.dumps(
+            {
+                "number_of_files": number_of_files,
+                "total_line_count": total_line_count,
+                "most_common_programming_languages": list_of_programming_languages,
+            })
+        
+
+        yield str(ready_for_analysis)
+        # Step 3: Identify sensitive code (filter unnecessary files with AI)
+        relevent_files = analysis.get_relevent_files(ready_for_analysis)
+        yield str(relevent_files)
+
+        logger.debug(f"Files identified as relevent : {relevent_files}")
+
+        # Step 4: Identify changes in the code (check for security issues with AI, and suggest first solutions)
+        in_depth_file_analysis = analysis.get_in_depth_file_analysis(json.loads(relevent_files).get("sensitive_files"))
+        yield str(in_depth_file_analysis)
+
+        logger.debug(f"Changes in code : {in_depth_file_analysis}")
+
+        # Step 3: Store the data in a SQLite database
+        # store_data_in_db(
+        #     DB_NAME, {"file_path": str(repository_url), "line_count": total_line_count}
+        # )
+
+        # logger.info(f"Processed {len(data)} files, representing {total_line_count} lines. Data stored in {db_name}.")
+    return StreamingResponse(repository_analysis(), media_type="text/event-stream")
diff --git a/app/routers/ws/__init__.py b/app/routers/ws/__init__.py
new file mode 100644
index 0000000..b4194ff
--- /dev/null
+++ b/app/routers/ws/__init__.py
@@ -0,0 +1 @@
+from .repositories import *
\ No newline at end of file
diff --git a/app/routers/ws/repositories.py b/app/routers/ws/repositories.py
new file mode 100644
index 0000000..20a9896
--- /dev/null
+++ b/app/routers/ws/repositories.py
@@ -0,0 +1,220 @@
+"""
+This file contains the code for the websocket endpoint.
+The websocket endpoint is used to create a websocket connection between the client and the server.
+The goal is to update the client frontend based on websocket messages created by events on the server.
+"""
+
+import asyncio
+import json
+import jwt
+import os
+from pathlib import Path
+from fastapi import APIRouter, WebSocket, Depends, WebSocketDisconnect
+from fastapi.responses import HTMLResponse
+from fastapi.security import OAuth2PasswordBearer
+
+from utils.websocket import WebSocketAPI
+from utils.analysis.files_analyser import format_github_url
+from utils import analysis
+from utils.databases import store_data_in_db
+
+from dependencies import get_token_header
+import logging
+
+logger = logging.getLogger(__name__)
+
+CLONE_DIR: Path = "cloned_repo"
+DB_NAME: str = "file_data.db"
+
+router = APIRouter(
+    prefix="/ws/repositories",
+    tags=["repositories"],
+    # dependencies=[Depends(get_token_header)],
+    # responses={404: {"description": "Not found"}},
+)
+from dotenv import load_dotenv
+
+load_dotenv()
+JWT_SECRET = os.getenv("SUPABASE_JWT_SECRET")
+
+
+html = """
+<!DOCTYPE html>
+<html>
+    <head>
+        <title>Chat</title>
+    </head>
+    <body>
+        <h1>WebSocket Chat</h1>
+        <form action="" onsubmit="sendMessage(event)">
+            <input type="text" id="messageText" autocomplete="off"/>
+            <button>Send me</button>
+        </form>
+        <ul id='messages'>
+        </ul>
+        <script>
+            var ws = new WebSocket("ws://localhost:8000/ws/repositories/analysis");
+            ws.onmessage = function(event) {
+                var messages = document.getElementById('messages')
+                var message = document.createElement('li')
+                var content = document.createTextNode(event.data)
+                message.appendChild(content)
+                messages.appendChild(message)
+            };
+            function sendMessage(event) {
+                var input = document.getElementById("messageText")
+                ws.send(input.value)
+                input.value = ''
+                event.preventDefault()
+            }
+
+        </script>
+    </body>
+</html>
+"""
+
+
+@router.get("/")
+async def get():
+    return HTMLResponse(html)
+
+
+@router.websocket("/message")
+async def websocket_endpoint(websocket: WebSocket):
+    await websocket.accept()
+    while True:
+        data = await websocket.receive_text()
+        await websocket.send_json({"success": f"Message text was: {data}"})
+
+
+@router.websocket("/test")
+async def ws_testing_endpoint(websocket: WebSocket):
+    """
+    Websocket endpoint for testing purposes.
+    """
+    await websocket.accept()
+    try:
+        for i in range(10):
+            message = f"Message {i}"
+            await websocket.send_json({"success": message})
+            await asyncio.sleep(1)  # Yield control back to the event loop messages
+    except WebSocketDisconnect:
+        print("Client disconnected")
+
+
+@router.websocket("/analysis")
+async def ws_repository_analysis(websocket: WebSocket):
+    """
+    Websocket endpoint for analysing a repository.
+    Submit data in realtime to the client, in order to update frontend.
+    """
+    await websocket.accept()
+    websocket_api = WebSocketAPI(websocket)
+    # We should create a queue and start processes in threads to avoid blocking the event loop
+    while True:
+        data = await websocket.receive_json()
+        await websocket_api.send_pending(message="Connecting to service")
+        logger.debug(f"Received data : {data}")
+        try:
+            repository_url = data["repositoryURL"]
+        except KeyError:
+            await websocket_api.send_error(message= "You should create an offer first")
+            continue
+        try:
+            token = data["token"]
+        except KeyError:
+            await websocket_api.send_error(message= "You should authenticate first")
+            continue
+        # Secure connection using supabase JWT token
+        try:
+            logger.error(f"Token : {token}" f"JWT_SECRET : {JWT_SECRET}")
+            payload = jwt.decode(
+                token, JWT_SECRET, algorithms=["HS256"], audience="authenticated"
+            )
+            logger.debug(f"Decoded payload : {payload}")
+            user_id = payload.get("sub")
+            # Check user_id correspond to the user who created the offer for selected repo
+        except Exception as error:
+            await websocket_api.send_error(message= f"An error has occured while decoding the token : {error}")
+            continue
+
+        # If we don't sleep, websocket is too fast and the client can't keep up
+        await websocket_api.send_success(message= f"Service ready for: {repository_url}")
+
+        # Make sure URL is in the right format
+        repository_url = format_github_url(repository_url)
+
+        # Step 1: Clone the repository
+        await websocket_api.send_pending(message= "Started cloning repository")
+        try:
+            analysis.clone_repo(repository_url, CLONE_DIR)
+        except Exception as error:
+            await websocket_api.send_error(message= f"An error has occured while cloning the repository : {error}")
+            continue
+        await websocket_api.send_success(message= f"Successfully cloned repository: {repository_url}")
+
+        await websocket_api.send_analyzing(message= "Started simple repository scan")
+        # Step 2: Process the repository to count files, lines, identify main languages
+        simple_repo_analysis = analysis.get_simple_repository_analysis(CLONE_DIR)
+        (
+            number_of_files,
+            total_line_count,
+            list_of_programming_languages,
+            ready_for_analysis,
+        ) = simple_repo_analysis
+
+        logger.debug(f"Number of files : {number_of_files}")
+        logger.debug(f"Total line count : {total_line_count}")
+        logger.debug(
+            f"Most common programming languages : {list_of_programming_languages}"
+        )
+        await websocket_api.send_success(
+                message= "Repository scan complete",
+                type="repositoryScan",
+                data= {
+                    "numberOfFiles": number_of_files,
+                    "totalLineCount": total_line_count,
+                    "mostCommonProgrammingLanguages": list_of_programming_languages,
+                },
+        )
+
+        await websocket_api.send_success(
+                 message= "Identified files relatives to project",
+                 type="relativeFiles",
+                data= {"relativeFiles":ready_for_analysis},
+        )
+        await websocket_api.send_analyzing(message= "Started in depth analysis")
+        # Step 3: Identify sensitive code (filter unnecessary files with AI)
+        sensitive_files = analysis.get_sensitive_files(ready_for_analysis)
+        await websocket_api.send_success(
+                message= "Identified sensitive files for in depth analysis",
+                type="sensitiveFiles",
+                data= sensitive_files,
+        )
+        logger.debug(f"Files identified as relevent : {sensitive_files}")
+
+        # Step 4: Identify changes in the code (check for security issues with AI, and suggest first solutions)
+        in_depth_file_analysis = analysis.get_in_depth_file_analysis(sensitive_files.get("sensitiveFiles"))
+        await websocket_api.send_success(
+                message="In depth analysis finished",
+                type="inDepthAnalysis",
+                data= in_depth_file_analysis,
+        )
+
+        # Step 5: Store the data in supabase database
+        store_data_in_db(
+            url=data["repositoryURL"],
+            files_count=number_of_files,
+            lines_count=total_line_count,
+        )
+
+        logger.debug(f"Changes in code : {in_depth_file_analysis}")
+
+        await websocket.close()
+        # Step 3: Store the data in a SQLite database
+        # store_data_in_db(
+        #     DB_NAME, {"file_path": str(repository_url), "line_count": total_line_count}
+        # )
+
+        # logger.info(f"Processed {len(data)} files, representing {total_line_count} lines. Data stored in {db_name}.")
+        return
diff --git a/app/utils/analysis/__init__.py b/app/utils/analysis/__init__.py
new file mode 100644
index 0000000..b036e37
--- /dev/null
+++ b/app/utils/analysis/__init__.py
@@ -0,0 +1 @@
+from .files_analyser import *
\ No newline at end of file
diff --git a/analysis/config/supported_extensions.csv b/app/utils/analysis/config/supported_extensions.csv
similarity index 100%
rename from analysis/config/supported_extensions.csv
rename to app/utils/analysis/config/supported_extensions.csv
diff --git a/app/utils/analysis/files_analyser.py b/app/utils/analysis/files_analyser.py
new file mode 100644
index 0000000..28a357c
--- /dev/null
+++ b/app/utils/analysis/files_analyser.py
@@ -0,0 +1,199 @@
+import json
+import os
+import logging
+import pandas as pd
+import chardet
+
+from pathlib import Path
+from typing import List, Tuple
+from collections import Counter
+from .ml import ChatGPTApi
+from git import Repo
+
+
+logging.basicConfig(
+    level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s"
+)
+logger = logging.getLogger(__name__)
+
+
+config_path = "app/utils/analysis/config/supported_extensions.csv"
+
+logger.debug("Loading GPT Model")
+model = ChatGPTApi()
+logger.debug("Model loaded")
+
+
+def get_important_programming_language(list_files: List[Path]) -> pd.DataFrame:
+    """Returns common extensions for the detected programming language"""
+    logger.debug("No extensions detected")
+    suffix_counts = Counter(map(lambda x: x.suffix, list_files))
+    most_common_suffix_list = suffix_counts.most_common()
+    logger.debug(f"Most common suffixes : {most_common_suffix_list}")
+    df = pd.read_csv(config_path)
+    # Find extensions which are in the list of supported extensions
+    # Count number of file for each extension
+    df["count"] = df["extension"].apply(
+        lambda x: suffix_counts[x] if x in suffix_counts else 0
+    )
+    df = df.sort_values(by="count", ascending=False)
+    # Remove extensions that are not in the list of supported extensions
+    df = df[df["count"] > 0]
+    logger.debug(df)
+    return df
+
+
+def get_sensitive_files(list_files: List[dict]) -> dict:
+    """Identify sensitive files using GPT
+    
+    return list of files {"path": str, "language": str}
+    """
+    logger.debug("Loading GPT Model")
+    logger.debug("Model loaded")
+    sensitive_files = model.identify_sensitive_files(list_files)
+    try:
+        # Try to format the data in json
+        sensitive_files = json.loads(sensitive_files)
+    except Exception as error:
+        logger.error(f"When identifying sensitive files an error has occured (likely GPT forgetting sensitive_files key) : {error}")
+        return []
+    logger.debug("All files analysed")
+    return sensitive_files
+
+
+def get_in_depth_file_analysis(list_files: List[List[dict]]):
+    """Asynchronously analyse each file with GPT to locate sensitive code
+    For each file it returns a list of issues which are dict with keys:
+    - lineNumber
+    - comment
+    - suggestion
+
+    We have to ensure we lead an analysis on relevent files (files that are likely to contain sensitive code)
+    """
+    in_depth_results = []
+    # Limit the number of files to 5 for testing
+    list_files = list_files[:5]
+    for file_data in list_files:
+        logger.error(file_data)
+        file_path = file_data.get("path")
+        try:
+            with open(file_path, "r") as file:
+                logger.info(f"Reading file : {file_path}")
+                # Put line numbers before each line so GPT can understand the context
+                content = list(
+                    map(
+                        lambda line: f"{line[0]}. {line[1]}" if line[1] != "" else None,
+                        enumerate(file.readlines(), 1),
+                    )
+                )
+                code = "".join(content)
+                logger.info(f"Code to analyze : {code}")
+                in_depth_result = model.in_depth_analysis(
+                    code, file_data.get("language")
+                )
+                try:
+                    # Try to format the data in json
+                    in_depth_result = json.loads(in_depth_result)
+                    # We should get something like {"issues":[]}
+                except Exception as error:
+                    logger.error(f"When identifying in depth file analysis an error has occured (likely GPT forgetting issues key) : {error}")
+                    continue # Goes to next file
+                logger.warning(f"Sensitive code found in file : {file_path}")
+                logger.warning(in_depth_result)
+                in_depth_result["path"] = file_path
+                in_depth_results.append(in_depth_result) # Add the analysis to the list
+        except Exception as error:
+            logger.error(f"An error has occured for file : {file_path} : {error}")
+    return in_depth_results
+
+
+def format_github_url(url: str) -> str:
+    """Format the URL to be used with the Github API"""
+    # Make sure there is github.com in the URL
+    if "github.com" not in url:
+        url = f"github.com/{url}"
+    # Make sure there is https:// in the URL
+    if "https://" not in url:
+        url = f"https://{url}"
+    return url
+
+def clone_repo(repo_url, clone_dir):
+    if not os.path.exists(clone_dir):
+        os.makedirs(clone_dir)
+    # Remove all files in the directory
+    for root, dirs, files in os.walk(clone_dir, topdown=False, followlinks=True):
+        for file in files:
+            logger.debug(f"Removing file {file}")
+            os.remove(os.path.join(root, file))
+        for dir in dirs:
+            logger.debug(f"Removing dir {dir}")
+            os.rmdir(os.path.join(root, dir))
+    # Clone the repository
+    Repo.clone_from(repo_url, clone_dir)
+
+
+# Function to count lines in a file
+def count_lines(file_path):
+    try:
+        with open(file_path, "rb") as file:
+            # TODO: Call GPT to identify not handled errors
+            raw_data = file.read()
+            result = chardet.detect(raw_data)
+            encoding = result["encoding"]
+            text = raw_data.decode(str(encoding))
+            lines = text.splitlines()
+            return len(lines)
+    except Exception as e:
+        print(f"Error reading {file_path}: {e}")
+        return 0
+
+
+def get_simple_repository_analysis(
+    clone_dir: str | Path,
+) -> Tuple[int, int, List[str], List[dict]]:
+    """Analyse the repository
+
+    - Get the list of files
+    - Count number of files
+    - Count number of lines
+    - Identify most common extensions
+    - Filter files with selected extensions
+
+    returns number_of_files, total_line_count, most_common_programming_languages, code_which_may_throw_error
+    """
+    if type(clone_dir) == str:
+        clone_dir = Path(clone_dir)
+    list_files_paths = list(clone_dir.rglob("*.*"))
+    number_of_files = len(list_files_paths)
+    total_line_count = 0
+    for file_path in list_files_paths:
+        total_line_count += count_lines(file_path)
+    important_programming_language = get_important_programming_language(
+        list_files_paths
+    )
+    list_of_important_extensions = important_programming_language["extension"].to_list()
+    # List of programming languages are going to be used for keywords
+    list_of_programming_languages = important_programming_language["name"].to_list()
+    logger.debug(f"Selected extensions : {list_of_important_extensions}")
+    # Filter files with selected extensions
+    selected_files = list(
+        filter(lambda x: x.suffix in list_of_important_extensions, list_files_paths)
+    )
+    logger.debug(f"Selected files : {selected_files}")
+    # Change list to dict with filepath and programming language as keys
+    ready_for_analysis = [
+        {
+            "path": str(file),
+            "language": important_programming_language[
+                important_programming_language["extension"] == file.suffix
+            ]["name"].values[0],
+        }
+        for file in selected_files
+    ]
+
+    return (
+        number_of_files,
+        total_line_count,
+        list_of_programming_languages,
+        ready_for_analysis,
+    )
diff --git a/app/utils/analysis/ml.py b/app/utils/analysis/ml.py
new file mode 100644
index 0000000..fd73c81
--- /dev/null
+++ b/app/utils/analysis/ml.py
@@ -0,0 +1,209 @@
+import os
+import logging
+from openai import OpenAI
+from typing import List
+
+
+logger = logging.getLogger(__name__)
+
+
+class ChatGPTApi:
+    """Class that is used to call chatgpt, you need to have your openai API key as an environemnt variable named OPENAI_API_KEY"""
+
+    def __init__(self) -> None:
+        assert (
+            os.getenv("OPENAI_API_KEY") is not None
+        ), "No API key detected, please setup your API key as an environement variable under the name OPENAI_API_KEY"
+        self.client = OpenAI()
+
+    def call(self, *, message):
+        response = self.client.chat.completions.create(
+            model="gpt-3.5-turbo-0125",
+            response_format={"type": "json_object"},
+            messages=message,
+        )
+        logger.debug(response)
+        return response.choices[0].message.content
+
+    def identify_sensitive_files(self, files: List[dict]):
+        """Identify sensitive files using GPT"""
+
+        message = [
+            {
+                "role": "system",
+                "content": (
+                    f"You will be provided with a list of files paths"
+                    "Your task is to identify which files are most likely to contain sensitive code, base your expectations on the path of the file."
+                    "Output is formatted as JSON with key sensitiveFiles containing a list of objects with keys:"
+                    "path, which is the path of the file containing sensitive code"
+                    "language, which is the programming language of the file"
+                    "understand that a main file is a file that is likely to contain the main logic of the application"
+                    "those types of files are usually the most sensitive ones."
+                ),
+            },
+            {
+                "role": "user",
+                "content": "[{'path': 'cloned_repo/main.py', 'language': 'Python'}]",
+            },
+            {
+                "role": "assistant",
+                "content": '{ "sensitiveFiles": [ { "path": "cloned_repo/main.py", "language": "Python" } ] }',
+            },
+            {
+                "role": "user",
+                "content": str(files),
+            },
+        ]
+        return self.call(message=message)
+
+    def in_depth_analysis(self, code: str, language: str = "python"):
+        """Analyse code in depth using GPT"""
+        if code is None or code == "":
+            return None
+        message = [
+            {
+                "role": "system",
+                "content": (
+                    f"You will be provided with a piece of {language} code"
+                    "Your task is to check code security."
+                    "If you find a possible security issue, you should provide a comment and a code suggestion to fix the issue (it must be code replacing existing one). "
+                    "Output is formatted as JSON with key issues containing a list of objects with keys:"
+                    "lineNumber, which is starting line where the issue occurs"
+                    "comment, which is a short description of the issue"
+                    "suggestion, which is a possible solution to the issue"
+                    "each entry of the list corresponds to a different issue in the code."
+                ),
+            },
+            # {
+            #     "role": "user",
+            #     "content": (
+            #         "1. import os\n"
+            #         "2. import sqlite3\n"
+            #         "3. import chardet\n"
+            #         "4. from git import Repo\n"
+            #         "5. from fastapi import FastAPI, BackgroundTasks\n"
+            #         "6. import uvicorn\n"
+            #         "7. from supabase import create_client, Client\n"
+            #         "8. from dotenv import load_dotenv\n"
+            #         "9. \n"
+            #         "10. load_dotenv()\n"
+            #         '11. url: str = os.environ.get("SUPABASE_URL")\n'
+            #         '12. key: str = os.environ.get("SUPABASE_KEY")\n'
+            #         "13. supabase: Client = create_client(url, key)\n"
+            #         "14. \n"
+            #         "15. # Create a FastAPI app\n"
+            #         "16. app = FastAPI()\n"
+            #         "17. \n"
+            #         "18. # Function to clone a Git repository\n"
+            #         "19. def clone_repo(repo_url, clone_dir):\n"
+            #         "20.     if not os.path.exists(clone_dir):\n"
+            #         "21.         os.makedirs(clone_dir)\n"
+            #         "22.     # Remove all files in the directory\n"
+            #         "23.     for root, dirs, files in os.walk(clone_dir, topdown=False, followlinks=True):\n"
+            #         "24.         for file in files:\n"
+            #         '25.             print(f"Removing file {file}")\n'
+            #         "26.             os.remove(os.path.join(root, file))\n"
+            #         "27.         for dir in dirs:\n"
+            #         "28. \n"
+            #         '29.             print(f"Removing dir /{dir}")\n'
+            #         "30.             os.rmdir(os.path.join(root, dir))\n"
+            #         "31.     # Clone the repository\n"
+            #         '32.     Repo.clone_from(f"https://github.com/{repo_url}", clone_dir)\n'
+            #         "33. \n"
+            #         "34. # Function to count lines in a file\n"
+            #         "35. def count_lines(file_path):\n"
+            #         "36.     try:\n"
+            #         '37.         with open(file_path, "rb") as file:\n'
+            #         "38.             #TODO: Call GPT to identify not handled errors\n"
+            #         "39.             raw_data = file.read()\n"
+            #         "40.             result = chardet.detect(raw_data)\n"
+            #         '41.             encoding = result["encoding"]\n'
+            #         "42.             text = raw_data.decode(str(encoding))\n"
+            #         "43.             lines = text.splitlines()\n"
+            #         "44.             return len(lines)\n"
+            #         "45.     except Exception as e:\n"
+            #         '46.         print(f"Error reading {file_path}: {e}")\n'
+            #         "47.         return 0\n"
+            #         "48. \n"
+            #         "49. # Function to process the repository and collect data\n"
+            #         "50. def process_repo(clone_dir):\n"
+            #         "51.     data = []\n"
+            #         "52.     for root, _, files in os.walk(clone_dir):\n"
+            #         "53.         # files_paths = [os.path.join(root, file) for file in files]\n"
+            #         "54.         # identify_project_type(files_paths)\n"
+            #         "55.         #TODO: Project: Python\n"
+            #         "56.         #TODO: Important extension: .py\n"
+            #         "57.         #TODO: Analysis of .py files\n"
+            #         "58.         #TODO: File sensible: main.py\n"
+            #         "59.         for file in files:\n"
+            #         "60.             file_path = os.path.join(root, file)\n"
+            #         "61.             if ('.git' in file_path):\n"
+            #         "62.                 continue\n"
+            #         "63.             line_count = count_lines(file_path)\n"
+            #         "64.             data.append((file_path, line_count))\n"
+            #         "65.     return data\n"
+            #         "66. \n"
+            #         "67. # Function to store data in a SQLite database\n"
+            #         "68. def store_data_in_db(*,url:str,files_count: int,lines_count: int):  \n"
+            #         "69.     response = (\n"
+            #         '70.     supabase.table("offers")\n'
+            #         '71.     .update({"files_count": files_count, "lines_count": lines_count})\n'
+            #         '72.     .eq("url", url)\n'
+            #         "73.     .execute()\n"
+            #         "74. )\n"
+            #         "75.     print(response)\n"
+            #         "76. \n"
+            #         "77. # Main function\n"
+            #         "78. def main(git_url: str):\n"
+            #         '79.     clone_dir = "cloned_repo"\n'
+            #         '80.     db_name = "file_data.db"\n'
+            #         "81. \n"
+            #         "82.     # Step 1: Clone the repository\n"
+            #         "83.     clone_repo(git_url, clone_dir)\n"
+            #         "84. \n"
+            #         "85.     # Step 2: Process the repository to count files and lines\n"
+            #         "86.     data = process_repo(clone_dir)\n"
+            #         "87.     lines_count = 0\n"
+            #         "88.     for file in data:\n"
+            #         "89.         lines_count += file[1]\n"
+            #         "90.     files_count = len(data)\n"
+            #         "91.     # Step 3: Store the data in a SQLite database\n"
+            #         "92.     store_data_in_db(url=git_url,files_count=files_count,lines_count=lines_count)\n"
+            #         "93. \n"
+            #         '94.     print(f"Processed {len(data)} files, representing {lines_count} lines. Data stored in {db_name}.")\n'
+            #         "95.     return data\n96. \n"
+            #         '97. @app.get("/")\n'
+            #         "98. def read_root(git_url: str, background_tasks: BackgroundTasks):\n"
+            #         '99.     print("Processing repository...")\n'
+            #         "100.     background_tasks.add_task(main, git_url)\n"
+            #         '101.     return {"message": "Processing repository in the background"}\n'
+            #         "102. \n"
+            #         '103. if __name__ == "__main__":\n'
+            #         '104.     uvicorn.run("main:app", host="0.0.0.0", port=8000, log_level="info")',
+            #     ),
+            # },
+            # {
+            #     "role": "assistant",
+            #     "content": (
+            #         '{'
+            #         '    "issues": ['
+            #                 '{'
+            #                 '    "line_number": 21,'
+            #                     '"comment": "Using os.makedirs to create directories insecurely",'
+            #                     '"suggestion": "Use os.makedirs with exist_ok=True to safely create directories"'
+            #                 '},'
+            #                 '{'
+            #                     '"line_number": 31,'
+            #                     '"comment": "Using os.rmdir to remove directories insecurely",'
+            #                     '"suggestion": "Use shutil.rmtree to safely remove directories"'
+            #                 '},'
+            #             ']'
+            #         '}'
+            #     )
+            # },
+            {
+                "role": "user",
+                "content": code,
+            },
+        ]
+        return self.call(message=message)
diff --git a/app/utils/databases.py b/app/utils/databases.py
new file mode 100644
index 0000000..da9838b
--- /dev/null
+++ b/app/utils/databases.py
@@ -0,0 +1,43 @@
+import os
+import sqlite3
+import logging
+from supabase import Client, create_client
+
+logger = logging.getLogger(__name__)
+
+url: str = os.environ.get("SUPABASE_URL")
+key: str = os.environ.get("SUPABASE_KEY")
+
+assert url is not None, "No SUPABASE_URL detected"
+assert key is not None, "No SUPABASE_KEY detected"
+
+supabase: Client = create_client(url, key)
+
+
+def store_data_in_db(db_name: str, data: dict):
+    conn = sqlite3.connect(db_name)
+    cursor = conn.cursor()
+    cursor.execute(
+        """CREATE TABLE IF NOT EXISTS file_data
+                      (id INTEGER PRIMARY KEY AUTOINCREMENT, 
+                       file_path TEXT, 
+                       line_count INTEGER)"""
+    )
+
+    cursor.executemany(
+        "INSERT INTO file_data (file_path, line_count) VALUES (?, ?)", data
+    )
+    conn.commit()
+    conn.close()
+
+
+# Function to store data in a supabase database
+def store_data_in_db(*, url: str, files_count: int, lines_count: int):
+    logger.error(f"Storing data in database: {url}, {files_count}, {lines_count}")
+    response = (
+        supabase.table("offers")
+        .update({"files_count": files_count, "lines_count": lines_count})
+        .eq("url", url)
+        .execute()
+    )
+    logger.error(f"Response from database: {response}")
\ No newline at end of file
diff --git a/app/utils/websocket.py b/app/utils/websocket.py
new file mode 100644
index 0000000..3d1b5d3
--- /dev/null
+++ b/app/utils/websocket.py
@@ -0,0 +1,38 @@
+import asyncio
+import logging
+
+logger = logging.getLogger(__name__)
+
+class WebSocketAPI:
+    def __init__(self, websocket):
+        self.websocket = websocket
+
+    async def send_success(self, *, message,type=None, data=None):
+        logger.debug(f"Sending success message: {message}")
+        logger.debug(f"Type: {type}")
+        logger.debug(f"Data: {data}")
+        payload = {"status": "success", "message": message}
+        if type is not None:
+            payload["type"] = type
+        if data is not None:
+            payload["data"] = data
+        await self.send_json(payload)
+        await self.yield_control()
+
+    async def send_analyzing(self, *, message):
+        await self.send_json({"status": "analyzing", "message": message})
+        await self.yield_control()
+
+    async def send_pending(self, *, message):
+        await self.send_json({"status": "pending", "message": message})
+        await self.yield_control()
+
+    async def send_error(self, *, message):
+        await self.send_json({"status": "error", "message": message})
+        await self.yield_control()
+
+    async def send_json(self, data):
+        await self.websocket.send_json(data)
+
+    async def yield_control(self):
+        await asyncio.sleep(0)  # Yield control back to the event loop
diff --git a/file_data.db b/file_data.db
new file mode 100644
index 0000000..c544b2c
Binary files /dev/null and b/file_data.db differ
diff --git a/main.py b/main.py
deleted file mode 100644
index 7b36ff8..0000000
--- a/main.py
+++ /dev/null
@@ -1,90 +0,0 @@
-import os
-import sqlite3
-import chardet
-from git import Repo
-from fastapi import FastAPI, BackgroundTasks
-import uvicorn
-from pathlib import Path
-from analysis.files_analyser import get_important_extensions,get_file_analysis
-from typing import Tuple,List
-
-import logging
-logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
-logger = logging.getLogger(__name__)
-
-#url: str = os.environ.get("SUPABASE_URL")
-#key: str = os.environ.get("SUPABASE_KEY")
-#supabase: Client = create_client(url, key)
-
-app = FastAPI()
-
-def clone_repo(repo_url, clone_dir):
-    if not os.path.exists(clone_dir):
-        os.makedirs(clone_dir)
-    # Remove all files in the directory
-    for root, dirs, files in os.walk(clone_dir, topdown=False, followlinks=True):
-        for file in files:
-            logger.info(f"Removing file {file}")
-            os.remove(os.path.join(root, file))
-        for dir in dirs:
-            print(f"Removing dir /{dir}")
-            os.rmdir(os.path.join(root, dir))
-    # Clone the repository
-    Repo.clone_from(repo_url, clone_dir)
-
-
-def process_repo(clone_dir : str | Path) -> Tuple[List[str],int]:
-    
-    if type(clone_dir) == str:
-        clone_dir = Path(clone_dir)
-    list_files = list(clone_dir.rglob("*.*"))
-
-    selected_extensions,_project_type = get_important_extensions(list_files)
-    selected_files = filter(lambda x : x.suffix in selected_extensions,list_files)
-    list_response = []
-    total_line_count = 0
-    for response,line_count in get_file_analysis(selected_files):
-        try:
-            list_response.append(response.choices[0].message.content)
-            total_line_count += line_count
-        except:
-            pass
-    return list_response,total_line_count
-
-def store_data_in_db(db_name, data):
-    conn = sqlite3.connect(db_name)
-    cursor = conn.cursor()
-    cursor.execute(
-        """CREATE TABLE IF NOT EXISTS file_data
-                      (id INTEGER PRIMARY KEY AUTOINCREMENT, 
-                       file_path TEXT, 
-                       line_count INTEGER)"""
-    )
-
-    cursor.executemany(
-        "INSERT INTO file_data (file_path, line_count) VALUES (?, ?)", data
-    )
-    conn.commit()
-    conn.close()
-
-def main(git_url: str):
-    clone_dir = "cloned_repo"
-    db_name = "file_data.db"
-    # Step 1: Clone the repository
-    clone_repo(git_url, clone_dir)
-    # Step 2: Process the repository to count files and lines
-    data,total_line_count = process_repo(clone_dir)
-    # Step 3: Store the data in a SQLite database
-    store_data_in_db(db_name, data)
-
-    logger.info(f"Processed {len(data)} files, representing {total_line_count} lines. Data stored in {db_name}.")
-    return data
-
-@app.get("/")
-def read_root(git_url: str, background_tasks: BackgroundTasks):
-    logger.info("Processing repository...")
-    background_tasks.add_task(main, git_url)
-    return {"message": "Processing repository in the background"}
-
-if __name__ == "__main__":
-    uvicorn.run("main:app", host="0.0.0.0", port=8000, log_level="info")
